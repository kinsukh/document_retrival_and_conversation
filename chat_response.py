import requests
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_chat_response(query, db_results):
    """
    Generate a conversational response based on the user's query and the database search results.
    This function uses the local Ollama API (http://localhost:1434/api/run) with the model 'llama3.2:latest'.
    
    Parameters:
        query (str): The user's search query.
        db_results (dict): Structured results from the Pinecone database.
    
    Returns:
        str: A conversation-like response generated by the LLM.
    """
    # Prepare the database context by concatenating content from the results
    db_context = ""
    if db_results and "pinecone" in db_results:
        for result in db_results["pinecone"]:
            db_context += result["content"] + "\n"
    
    # Create the prompt combining user query and database context
    prompt = (
        f"User Query: {query}\n"
        f"Database Context:\n{db_context}\n"
        "Chatbot Response:"
    )
    
    # Define the Ollama API endpoint and payload
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": "llama3.2:latest",
        "prompt": prompt,
        "options" :{
            "num_predict":150,
            "temperature":0.7
        },
        "stream" : False
        
    }
    
    try:
        logger.info("Sending request to Ollama API at %s", url)
        response = requests.post(url, json=payload)
        response.raise_for_status()
        data = response.json()
        generated_text = data.get("response", "").strip()
        if not generated_text:
            logger.error("No generated response from Ollama API")
            return "I'm sorry, I couldn't generate a proper response using Ollama."
        return generated_text
    except requests.exceptions.RequestException as e:
        logger.error("Error communicating with Ollama API: %s", e)
        return "I'm sorry, there was an error contacting the Ollama API."

# --------------------------------------------
# Previous implementation using subprocess (commented out):
#
# import subprocess
# import logging
#
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)
#
# def get_chat_response(query, db_results):
#     db_context = ""
#     if db_results and "pinecone" in db_results:
#         for result in db_results["pinecone"]:
#             db_context += result["content"] + "\n"
#     prompt = f"User Query: {query}\nDatabase Context:\n{db_context}\nChatbot Response:"
#     
#     command = f'ollama run llama3.2:latest --prompt "{prompt}"'
#     
#     try:
#         logger.info("Executing command: %s", command)
#         result = subprocess.run(command, capture_output=True, text=True, shell=True, check=True)
#         response = result.stdout.strip()
#         if not response:
#             logger.error("No response received from Ollama.")
#             return "I'm sorry, I couldn't generate a proper response using Ollama."
#         return response
#     except subprocess.CalledProcessError as cpe:
#         logger.error("Ollama command failed with error: %s", cpe.stderr)
#         return "I'm sorry, there was an error processing your request with Ollama."
#     except Exception as e:
#         logger.error("Error generating chat response using Ollama: %s", e)
#         return "I'm sorry, I couldn't generate a proper response using Ollama."
